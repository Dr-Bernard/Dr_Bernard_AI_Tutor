# First I leveraged on the os module to interact with the environment variables. This is a common practice to manage configurations, especially sensitive data like API keys.

import os

# Let us now import Groq class from the groq module - the client library for interfacing with Groq's AI services.

from groq import Groq

# I imported the load_dotenv from the dotenv library so that I can load environment variables from my .env file

from dotenv import load_dotenv

# Loading the environment variables using load_dotenv() which is a Python function that loads environment variables from a `.env` file.

load_dotenv()

# I instantiated the Groq client, providing the API key obtained from the environment. This securely authenticates my requests to the Groq API.

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Here I created a chat completion request - constructing a request for the chat completion endpoint. The payload includes a message with the role set to "user" and the content containing my query. I specified the model llama3-8b-8192, which is a specific pre-trained large language model optimized for this kind of task.

# chat_completion = client.chat.completions.create(
#     messages=[
#         {
#             "role": "user",
#             "content": "What is AGI",
#         }
#     ],
#     model="llama3-8b-8192",
# )

# # I now accessed the first choice in the chat_completion response and printed out the message content. Here, the API returns a structured response with multiple choices, each containing a message object
# print(chat_completion.choices[0].message.content)

while True:
    user_input = input("User: ")
    if user_input == "quit":
        break

    chat_completion = client.chat.completions.create(
        messages=[
            {"role": "user", "content": user_input}
        ],
        model="llama3-8b-8192",
    )


    print("Chatbot: " + chat_completion.choices[0].message.content)